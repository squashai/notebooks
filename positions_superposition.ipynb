{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "positions-superposition.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "18YEK-Np1q8DSOjcv_BkzsTKx7p5HFsZD",
      "authorship_tag": "ABX9TyM6nwWM0Hw9cWgnlxsl+piA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/squashai/notebooks/blob/main/positions_superposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygoII-vrKGbA"
      },
      "source": [
        "# Superposition of court positions of squash players in random videos\n",
        "\n",
        "This notebook extracts player positions from a squash video using the convolutional neural network created and trained in [baseline-court-positions](https://github.com/squashai/notebooks/blob/main/baseline_court_positions.ipynb), and assembles a video with such positions superposed to the original one.\n",
        "\n",
        "[![SQUASHAI](https://img.shields.io/badge/SQUASH-AI-black)](https://squashai.github.io)\n",
        "[![View source on Github](https://img.shields.io/badge/-View%20source%20on%20Github-blue?logo=github&labelColor=black)](https://github.com/squashai/notebooks/blob/main/positions_superposition.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSysVo7tbgd_"
      },
      "source": [
        "# Imports and Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFJfFVeLfHDV"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIIEdEENcxTn"
      },
      "source": [
        "VIDEO_URL = 'https://www.dropbox.com/s/f79eakp3u2lvo5e/squash-rally.mp4?dl=1'\n",
        "MODEL_URL = 'https://github.com/squashai/notebooks/releases/download/0.0.1/best_bl-positions_model.hdf5'\n",
        "BACKGROUND_URL = 'https://raw.githubusercontent.com/squashai/squashai/master/src/assets/ground.png'\n",
        "\n",
        "VIDEO_FILE = 'squash-rally.mp4'\n",
        "MODEL_FILE = 'best_bl-positions_model.hdf5'\n",
        "BACKGROUND_FILE = 'ground.png'\n",
        "OUTPUT_FILE = 'enanched-squash-rally.mp4'\n",
        "\n",
        "MODEL_INPUT_SIZE = (108, 192)\n",
        "\n",
        "COURT_SIZE = (195, 128)\n",
        "\n",
        "OUTPUT_OVERLAY_OFFSET = (20, 20)\n",
        "OUTPUT_OVERLAY_SIZE = (487, 320)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmnefLfUYy_v"
      },
      "source": [
        "# Data Retrivial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_gIrZqJeMMx"
      },
      "source": [
        "%cd /content/\n",
        "!curl -L --output $VIDEO_FILE $VIDEO_URL\n",
        "!curl -L --output $MODEL_FILE $MODEL_URL\n",
        "!curl -L --output $BACKGROUND_FILE $BACKGROUND_URL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-PhXD2MNQAX"
      },
      "source": [
        "# Prediction and Output Video Composition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STcYaRi7mfJH"
      },
      "source": [
        "model = load_model(MODEL_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yN8veObo_Jz"
      },
      "source": [
        "def scale_image(image, output_size):\n",
        "    h, w = image.shape[:2]\n",
        "    if isinstance(output_size, int):\n",
        "        if h > w:\n",
        "            new_h, new_w = output_size * h / w, output_size\n",
        "        else:\n",
        "            new_h, new_w = output_size, output_size * w / h\n",
        "    else:\n",
        "        new_h, new_w = output_size\n",
        "\n",
        "    new_h, new_w = int(new_h), int(new_w)\n",
        "    new_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "    return new_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wlWexrT8fpX"
      },
      "source": [
        "background = np.array(mpimg.imread(BACKGROUND_FILE))\n",
        "#background = cv2.resize(bg, COURT_SIZE[::-1], interpolation=cv2.INTER_AREA)\n",
        "background = scale_image(background, COURT_SIZE)\n",
        "\n",
        "def prediction_to_frame(prediction):\n",
        "\n",
        "    overlay = prediction[0, :COURT_SIZE[0], :COURT_SIZE[1], 0]\n",
        "    overlay -= np.min(overlay)\n",
        "    overlay /= np.max(overlay)\n",
        "\n",
        "    frame = np.array(background[:, :, :3])\n",
        "    frame[:, :, 0] = np.maximum(frame[:, :, 0] * (1 - overlay), overlay)\n",
        "    frame[:, :, 1] = np.maximum(frame[:, :, 1] * (1 - overlay), overlay)\n",
        "    #frame[:, :, 2] = np.maximum(frame[:, :, 2] * (1 - overlay), overlay)\n",
        "\n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqzpdX49-_W6"
      },
      "source": [
        "source_video = cv2.VideoCapture(VIDEO_FILE)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "framerate = source_video.get(cv2.CAP_PROP_FPS)\n",
        "size = (int(source_video.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        int(source_video.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "destination_video = cv2.VideoWriter(OUTPUT_FILE, fourcc, framerate, size)\n",
        "\n",
        "success, source_frame = source_video.read()\n",
        "while success:\n",
        "    model_input = scale_image(source_frame, MODEL_INPUT_SIZE)\n",
        "    model_input = np.array([cv2.cvtColor(model_input, cv2.COLOR_BGR2RGB)])\n",
        "    model_prediction = model.predict(model_input)\n",
        "    court_frame = prediction_to_frame(model_prediction)\n",
        "\n",
        "    court_frame = scale_image(court_frame, OUTPUT_OVERLAY_SIZE)\n",
        "    x_min = OUTPUT_OVERLAY_OFFSET[0]\n",
        "    x_max = OUTPUT_OVERLAY_OFFSET[0] + court_frame.shape[0]\n",
        "    y_min = OUTPUT_OVERLAY_OFFSET[1]\n",
        "    y_max = OUTPUT_OVERLAY_OFFSET[1] + court_frame.shape[1]\n",
        "\n",
        "    destination_frame = cv2.cvtColor(source_frame, cv2.COLOR_BGR2RGB)\n",
        "    court_frame = np.array(court_frame * 255, dtype=np.uint8)\n",
        "    destination_frame[x_min:x_max, y_min:y_max, :] = court_frame[:, :, :]\n",
        "\n",
        "    destination_video.write(cv2.cvtColor(destination_frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    success, source_frame = source_video.read()\n",
        "\n",
        "source_video.release()\n",
        "destination_video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRrdAKezLdJt"
      },
      "source": [
        "!cp $OUTPUT_FILE /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}